{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae35230f",
   "metadata": {},
   "source": [
    "## 저장된 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a33d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import PIL\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "sample = torch.load(os.path.join(os.getcwd(), 'resnet18_weight/checkpoint/ckpt.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb6b4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=512, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "resnet18 = models.resnet18(pretrained = False)\n",
    "\n",
    "resnet18.fc = nn.Linear(512, 2)\n",
    "print(resnet18.fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cbd4a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18.load_state_dict(sample['net'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cdb084",
   "metadata": {},
   "source": [
    "## CAM 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90dc300b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과 저장 위치:  ./2021-05-10-21:40\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4876, 0.4544, 0.4165), (0.2257, 0.2209, 0.2212)),\n",
    "])\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(root = os.path.join(os.getcwd(), 'CatvsDog_test'), transform = transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 1, shuffle = True, num_workers = 0)\n",
    "\n",
    "\n",
    "current_time = datetime.datetime.now() + datetime.timedelta(hours= 9)\n",
    "current_time = current_time.strftime('%Y-%m-%d-%H:%M')\n",
    "\n",
    "\n",
    "saved_loc = os.path.join('./', current_time)\n",
    "if os.path.exists(saved_loc):\n",
    "    shutil.rmtree(saved_loc)\n",
    "os.mkdir(saved_loc)\n",
    "\n",
    "print(\"결과 저장 위치: \", saved_loc)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5717c1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35cd3f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label : 0, Predicted label : 0, Probability : 1.00\n",
      "True label : 1, Predicted label : 1, Probability : 1.00\n",
      "True label : 0, Predicted label : 0, Probability : 1.00\n",
      "True label : 1, Predicted label : 1, Probability : 1.00\n",
      "True label : 1, Predicted label : 1, Probability : 0.99\n"
     ]
    }
   ],
   "source": [
    "# final conv layer name \n",
    "finalconv_name = 'layer4'\n",
    "\n",
    "# inference mode\n",
    "resnet18.eval()\n",
    "\n",
    "# number of result\n",
    "num_result = 5\n",
    "\n",
    "\n",
    "feature_blobs = []\n",
    "backward_feature = []\n",
    "\n",
    "# output으로 나오는 feature를 feature_blobs에 append하도록\n",
    "def hook_feature(module, input, output):\n",
    "    feature_blobs.append(output.cpu().data.numpy())\n",
    "    \n",
    "# Grad-CAM\n",
    "def backward_hook(module, input, output):\n",
    "    backward_feature.append(output[0])\n",
    "\n",
    "    \n",
    "resnet18._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "resnet18._modules.get(finalconv_name).register_backward_hook(backward_hook)\n",
    "\n",
    "\n",
    "# get the softmax weight\n",
    "params = list(resnet18.parameters())\n",
    "weight_softmax = np.squeeze(params[-2].cpu().detach().numpy()) # [2, 512]\n",
    "\n",
    "\n",
    "# generate the class activation maps\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    size_upsample = (224, 224)\n",
    "    _, nc, h, w = feature_conv.shape # nc : number of channel, h: height, w: width\n",
    "    output_cam = []\n",
    "    # weight 중에서 class index에 해당하는 것만 뽑은 다음, 이를 conv feature와 곱연산\n",
    "    cam = weight_softmax[class_idx].dot(feature_conv.reshape((nc, h*w))) \n",
    "    cam = cam.reshape(h, w)\n",
    "    cam = cam - np.min(cam)\n",
    "    cam_img = cam / np.max(cam)\n",
    "    cam_img = np.uint8(255 * cam_img)\n",
    "    output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "\n",
    "for i, (image, target) in enumerate(test_loader):\n",
    "    \n",
    "    # 모델의 input으로 주기 위한 image는 따로 설정\n",
    "    image_for_model = image.clone().detach()\n",
    "\n",
    "    # Image denormalization, using mean and std that i was used.\n",
    "    image[0][0] *= 0.2257\n",
    "    image[0][1] *= 0.2209\n",
    "    image[0][2] *= 0.2212\n",
    "    \n",
    "    image[0][0] += 0.4876\n",
    "    image[0][1] += 0.4544\n",
    "    image[0][2] += 0.4165\n",
    "    \n",
    "\n",
    "    # 모델의 input으로 사용하도록.\n",
    "    image_tensor = image_for_model.to(device)\n",
    "    logit = resnet18(image_tensor)\n",
    "    h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "    \n",
    "    probs, idx = h_x.sort(0, True)\n",
    "    print(\"True label : %d, Predicted label : %d, Probability : %.2f\" % (target.item(), idx[0].item(), probs[0].item()))\n",
    "    \n",
    "    \n",
    "    CAMs = returnCAM(feature_blobs[0], weight_softmax, [idx[0].item()]) # CAMs[0]: (224, 224), numpy\n",
    "    heatmap = cv2.applyColorMap(CAMs[0], cv2.COLORMAP_JET) # (224, 224, 3), numpy\n",
    "    heatmap = torch.from_numpy(heatmap).permute(2, 0, 1).float().div(255) # (3, 244, 244), torch.Tensor\n",
    "    blue, green, red = heatmap.split(1)\n",
    "    heatmap = torch.cat([red, green, blue]) # (3, 244, 244), opencv's default format is BGR, so we need to change it as RGB format.\n",
    "    \n",
    "    save_image(heatmap, os.path.join(saved_loc, \"Class_Activation_Map_%d.jpg\" % (i+1)))\n",
    "    \n",
    "    result = heatmap + image.cpu() # (1, 3, 224, 224)\n",
    "    result = result.div(result.max()).squeeze() # (3, 224, 224)\n",
    "    \n",
    "    save_image(result, os.path.join(saved_loc, \"heatmap&image_%d.jpg\" % (i+1)))\n",
    "    \n",
    "    \n",
    "    # ============================= #\n",
    "    # ==== Grad-CAM main lines ==== #\n",
    "    # ============================= #\n",
    "    \n",
    "    score = logit[:, idx[0]].squeeze() # 예측값 y^c\n",
    "    score.backward(retain_graph = True) # 예측값 y^c에 대해서 backprop 진행\n",
    "    \n",
    "    activations = torch.Tensor(feature_blobs[0]).to(device) # (1, 512, 7, 7), forward activations\n",
    "    gradients = backward_feature[0] # (1, 512, 7, 7), backward gradients\n",
    "    b, k, u, v = gradients.size()\n",
    "    \n",
    "    alpha = gradients.view(b, k, -1).mean(2) # (1, 512, 7*7) => (1, 512), feature map k의 'importance'\n",
    "    weights = alpha.view(b, k, 1, 1) # (1, 512, 1, 1)\n",
    "    \n",
    "    grad_cam_map = (weights*activations).sum(1, keepdim = True) # alpha * A^k = (1, 512, 7, 7) => (1, 1, 7, 7)\n",
    "    grad_cam_map = F.relu(grad_cam_map) # Apply R e L U\n",
    "    grad_cam_map = F.interpolate(grad_cam_map, size=(224, 224), mode='bilinear', align_corners=False) # (1, 1, 224, 224)\n",
    "    map_min, map_max = grad_cam_map.min(), grad_cam_map.max()\n",
    "    grad_cam_map = (grad_cam_map - map_min).div(map_max - map_min).data # (1, 1, 224, 224), min-max scaling\n",
    "\n",
    "    # grad_cam_map.squeeze() : (224, 224)\n",
    "    grad_heatmap = cv2.applyColorMap(np.uint8(255 * grad_cam_map.squeeze().cpu()), cv2.COLORMAP_JET) # (224, 224, 3), numpy \n",
    "    grad_heatmap = torch.from_numpy(grad_heatmap).permute(2, 0, 1).float().div(255) # (3, 224, 224)\n",
    "    b, g, r = grad_heatmap.split(1)\n",
    "    grad_heatmap = torch.cat([r, g, b]) # (3, 244, 244), opencv's default format is BGR, so we need to change it as RGB format.\n",
    "\n",
    "    save_image(grad_heatmap, os.path.join(saved_loc, \"Grad_CAM_%d.jpg\" % (i+1)))\n",
    "    \n",
    "    grad_result = grad_heatmap + image.cpu() # (1, 3, 224, 224)\n",
    "    grad_result = grad_result.div(grad_result.max()).squeeze() # (3, 224, 224)\n",
    "    \n",
    "    save_image(grad_result, os.path.join(saved_loc, \"GradCAM&image_%d.jpg\" % (i+1)))\n",
    "    \n",
    "    \n",
    "    image_list = []\n",
    "    \n",
    "    image_list.append(torch.stack([image.squeeze().cpu(), heatmap, grad_heatmap, result, grad_result], 0)) # (5, 3, 224, 224)\n",
    "    \n",
    "    images = make_grid(torch.cat(image_list, 0), nrow = 5)\n",
    "    \n",
    "    save_image(images, os.path.join(saved_loc, \"Final_Result_%d.jpg\" % (i+1)))\n",
    "    \n",
    "\n",
    "    if i + 1 == num_result:\n",
    "        break\n",
    "        \n",
    "    feature_blobs.clear()\n",
    "    backward_feature.clear()\n",
    "\n",
    "feature_blobs.clear()\n",
    "backward_feature.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b156b69f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
